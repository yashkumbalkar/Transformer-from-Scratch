## ðŸ“ Project Structure

```
â”œâ”€â”€ config.py                      # Configuration parameters for model and training
â”œâ”€â”€ input_embedding.py             # Token + positional embedding layer
â”œâ”€â”€ multi_head_attention.py        # Multi-head self-attention mechanism
â”œâ”€â”€ layer_norm.py                  # Layer normalization module
â”œâ”€â”€ feed_forward.py                # Feed-forward network used inside encoder blocks
â”œâ”€â”€ encoder.py                     # Transformer encoder block and stack
â”œâ”€â”€ transformer_classifier.py      # Encoder-only model architecture for classification
â”œâ”€â”€ tokenizer.py                   # Text tokenizer and preprocessing utility
â”œâ”€â”€ dataset.py                     # Dataset loading and preparation (for emotion detection)
â”œâ”€â”€ train_utils.py                 # Utilities for training (schedulers, metrics, etc.)
â”œâ”€â”€ train.py                       # Training script for the classifier
â”œâ”€â”€ inference.py                   # Inference script for predicting emotions from text
```



# ðŸš€ Transformer Architectures from Scratch in PyTorch

This repository contains two major implementations of the Transformer architecture:

1. **Full Transformer (Encoder-Decoder)** â€” A clean, from-scratch PyTorch implementation of the original [*Attention is All You Need*](https://arxiv.org/abs/1706.03762) paper.
2. **Encoder-Only Transformer for Emotion Detection** â€” A custom-built encoder-only transformer model trained on a text-based emotion classification task.

Both models are implemented without using high-level modules like `torch.nn.Transformer`, allowing for full transparency and learning of the underlying mechanics of transformer-based architectures.

---

## ðŸ“ File Structure




---

## ðŸ“Œ 1. Full Transformer (Encoder-Decoder)

- Implements all core components:
  - Scaled Dot-Product Attention
  - Multi-Head Attention
  - Positional Encoding
  - Encoder & Decoder blocks
  - Final linear + softmax layer
- Follows architecture described in *"Attention is All You Need"* (Vaswani et al., 2017)
- No external libraries like `torch.nn.Transformer` or `transformers` used

ðŸ›  Ideal for:
- Learning internals of transformer architecture
- Educational demonstrations
- Extending into other seq2seq tasks (e.g., translation, summarization)

> ðŸ—‚ Note: This model is implemented as a base and **not trained** on a specific task in this repo.

---

## ðŸ“Œ 2. Encoder-Only Transformer for Emotion Detection

- Built using the encoder stack from the original transformer
- Trained on a text classification task for **emotion detection**
- Pipeline includes:
  - Custom tokenizer
  - Embedding & encoder layers
  - Classification head
  - Training and evaluation scripts

ðŸŽ¯ Task: Predict the emotion (e.g., joy, anger, sadness, etc.) expressed in a sentence.

ðŸ§ª Sample usage (inference):

```python
from inference import predict_emotion

text = "I can't believe how happy I am today!"
emotion = predict_emotion(text)
print(f"Predicted Emotion: {emotion}")



